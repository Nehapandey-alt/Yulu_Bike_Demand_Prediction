{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "EM7whBJCYoAo",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "22aHeOlLveiV",
        "8yEUt7NnHlrM",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "49K5P_iCpZyH",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**  -  Regression - Yulu Bike Sharing Demand Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on predicting Yulu Bike rental demand using machine learning techniques. The dataset included hourly rental counts along with variables such as weather, season, holiday, and time-based features. Through Exploratory Data Analysis (EDA), we observed that demand peaks during commuting hours and varies significantly with temperature, rainfall, and seasonality.\n",
        "\n",
        "After preprocessing steps like feature engineering, encoding, scaling, and multicollinearity checks, two models were implemented – Linear Regression and Random Forest Regressor. While Linear Regression served as a baseline, the Random Forest model provided much higher accuracy.\n",
        "\n",
        "Feature importance analysis highlighted hour of the day, temperature, season, and humidity as the strongest predictors. The results will help Yulu Bikes in demand forecasting, fleet optimization, and enhancing customer satisfaction, contributing to both operational efficiency and sustainable urban mobility.."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the rapidly growing landscape of urban transportation, bike-sharing services such as Yulu Bike are becoming a vital solution for sustainable and affordable mobility. However, one of the biggest challenges faced by these companies is the uncertainty in demand. Bike rentals fluctuate based on multiple factors such as time of day, weather conditions, seasons, holidays, and working days.\n",
        "\n",
        "Without accurate demand forecasting, the company may face two major issues:\n",
        "\n",
        "Unavailability of bikes during peak hours, leading to customer dissatisfaction.\n",
        "\n",
        "Idle or underutilized bikes during low-demand periods, resulting in operational inefficiency.\n",
        "\n",
        "The objective of this project is to develop a machine learning model to accurately predict the hourly bike rental demand using historical and environmental data. This will enable Yulu Bike to optimize fleet management, improve customer satisfaction, and enhance operational efficiency, thereby supporting sustainable urban mobility."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jnYPjM5ltZDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions\n"
      ],
      "metadata": {
        "id": "WU7mrtwwpxEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import contractions\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "file_id = \"1dZ7p614gC_iwxHwcj-1N0Lc155AGMTJS\"\n",
        "download_url = f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "\n",
        "# Try with common encodings\n",
        "df = pd.read_csv(download_url, encoding=\"latin1\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# No missing values found"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Seoul Bike Sharing Dataset contains daily and hourly records of bike rentals in Seoul city, where the main goal is to predict the Rented Bike Count based on time, weather, and seasonal factors. It includes features such as date, hour, temperature, humidity, wind speed, visibility, solar radiation, rainfall, snowfall, season, holiday, and functioning day, all of which significantly influence bike demand. The dataset shows clear time-series patterns, with higher rentals during peak commuting hours and specific seasons, while weather conditions like rainfall and snowfall reduce demand. Since the data contains both numerical and categorical features, encoding, feature scaling, and handling skewness are necessary before modeling. This makes it a supervised regression problem, where machine learning models like Linear Regression, Random Forest, or Gradient Boosting can be applied to accurately forecast bike demand, providing valuable insights for operational planning and resource allocation."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "unique_values = df.nunique()\n",
        "unique_values"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Convert 'Date' column to datetime format with the correct format\n",
        "df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y', errors='coerce')\n",
        "\n",
        "# Extract year, month, day, and day of week\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Day'] = df['Date'].dt.day\n",
        "df['DayofWeek'] = df['Date'].dt.dayofweek # Monday=0, Sunday=6\n",
        "\n",
        "# Drop the original 'Date' column as we have extracted the necessary information\n",
        "df = df.drop('Date', axis=1)\n",
        "\n",
        "# Display the first few rows with the new columns\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset ko analysis aur modeling ke liye ready karne ke liye pehle column names ko standardize kiya gaya (spaces remove karke lowercase format). Date column ko datetime type me convert kiya aur categorical variables jaise Holiday aur Functioning Day ko numeric (Yes=1, No=0) me encode kiya. Missing values ka check kiya gaya aur forward fill method se handle kiya, jabki duplicates remove kiye gaye. Kuch features jaise Rainfall, Snowfall aur Solar Radiation me heavy skewness tha, jise log transformation se normalize kiya gaya. Insights ke taur par data se ye samajh aaya ki bike rentals weekdays pe zyada hote hain aur holidays ya heavy rainfall/snowfall ke din kaafi kam hote hain. Temperature aur bike demand ke beech me strong positive relation mila, jabki humidity aur wind speed ka demand par negative impact dekha gaya. Overall, cleaned dataset ab exploratory analysis aur machine learning modeling ke liye ready hai."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Histogram of Rented Bike Count"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df['Rented Bike Count'], bins=30, kde=True, color=\"skyblue\")\n",
        "plt.title(\"Distribution of Rented Bike Count\")\n",
        "plt.xlabel(\"Rented Bike Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram is best for showing the distribution of a numerical variable.\n",
        "\n",
        "Here, it helps us understand whether bike demand is normally distributed, skewed, or has multiple peaks."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most rentals are concentrated around low-to-medium values.\n",
        "\n",
        "Demand is right-skewed → very high rentals are less frequent, but they do occur (e.g., during rush hours or favorable weather)."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive: Helps in capacity planning → Yulu knows that most of the time demand is moderate, so bikes should be distributed accordingly.\n",
        "\n",
        "⚠️ Negative growth risk: If demand distribution shows a lot of zero or very low rentals, it may indicate underutilization of resources, leading to operational losses."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Line Plot (Date vs Count) → Long-term trend"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Reconstruct 'Date' column from Year, Month, and Day\n",
        "df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
        "daily_trend = df.groupby(\"Date\")[\"Rented Bike Count\"].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.lineplot(data=daily_trend, x=\"Date\", y=\"Rented Bike Count\", color=\"green\")\n",
        "plt.title(\"Daily Trend of Bike Rentals\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Total Rented Bikes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line charts are ideal for time-series trends.\n",
        "\n",
        "Here, it shows how demand changes daily across months."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clear seasonal fluctuations: demand increases in summer/autumn, decreases in winter.\n",
        "\n",
        "Demand also shows weekly waves (commuting cycle)."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive: Helps in forecasting demand by season → more bikes during high demand (summer/autumn), fewer in low demand (winter).\n",
        "\n",
        "⚠️ Negative growth risk: If demand keeps falling over months, it signals customer dissatisfaction or external competition."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 *Boxplot* (Hour vs Count) → Commuting pattern"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=df, x=\"Hour\", y=\"Rented Bike Count\", palette=\"Set2\")\n",
        "plt.title(\"Hourly Bike Rental Pattern (Commuting Behavior)\")\n",
        "plt.xlabel(\"Hour of the Day\")\n",
        "plt.ylabel(\"Rented Bike Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplots are excellent for showing distribution across categories (here, hours of the day).\n",
        "\n",
        "It helps to identify commuting patterns and outliers."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Demand peaks around 8 AM (office commute) and 5–8 PM (return commute).\n",
        "\n",
        "Very low demand during late night (0–5 AM)."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive: Yulu can reallocate bikes during peak hours in business districts to maximize rentals.\n",
        "\n",
        "⚠️ Negative growth: If not enough bikes are available during peaks, customers may switch to competitors (missed revenue opportunity)."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Line Plot (Month vs Avg Count)"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "monthly_trend = df.groupby(\"Month\")[\"Rented Bike Count\"].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.lineplot(data=monthly_trend, x=\"Month\", y=\"Rented Bike Count\", marker=\"o\", color=\"blue\")\n",
        "plt.title(\"Average Monthly Bike Rentals\")\n",
        "plt.xlabel(\"Month (1 = Jan, ..., 12 = Dec)\")\n",
        "plt.ylabel(\"Average Rented Bikes\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line plots are best to capture monthly/seasonal trends over time.\n",
        "\n",
        "It helps to clearly visualize whether demand is increasing, decreasing, or following a seasonal cycle."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rentals are higher in summer and autumn months.\n",
        "\n",
        "Demand is lowest in winter months, likely due to cold weather."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive: Helps in fleet planning → more bikes can be deployed during peak months, fewer in low-demand months (reduces idle bikes).\n",
        "\n",
        "⚠️ Negative growth: If winter demand keeps dropping drastically, it may reduce revenue stability. Yulu may need promotional offers in low season."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Bar Plot (Seasons vs Avg Count)"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "seasonal_trend = df.groupby(\"Seasons\")[\"Rented Bike Count\"].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=seasonal_trend, x=\"Seasons\", y=\"Rented Bike Count\", palette=\"Set2\")\n",
        "plt.title(\"Average Rentals per Season\")\n",
        "plt.xlabel(\"Season\")\n",
        "plt.ylabel(\"Average Rented Bikes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar plots are ideal for comparing categories (here, Seasons).\n",
        "\n",
        "It gives a quick summary of how different seasons affect bike demand."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autumn and Summer have the highest average rentals.\n",
        "\n",
        "Winter has the lowest demand."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive: Clear seasonal insights allow Yulu to adjust marketing campaigns and offers based on seasonality.\n",
        "\n",
        "⚠️ Negative growth: If Yulu doesn’t adapt in winter (low demand), it may face revenue dips due to excess idle bikes and high maintenance costs."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 Boxplot (Weekday vs Count)"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=df, x=\"DayofWeek\", y=\"Rented Bike Count\", palette=\"coolwarm\")\n",
        "plt.title(\"Weekday vs Weekend Rentals Distribution\")\n",
        "plt.xlabel(\"Day of the Week\")\n",
        "plt.ylabel(\"Rented Bike Count\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplots effectively show variation in demand across weekdays vs weekends.\n",
        "\n",
        "It helps detect commuting vs leisure patterns."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rentals are higher on weekdays, especially during office commute hours.\n",
        "\n",
        "Weekends show relatively lower demand, indicating casual/recreational use."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive: Yulu can allocate more bikes to office areas during weekdays, and shift some fleet to parks/leisure areas on weekends.\n",
        "\n",
        "⚠️ Negative growth: If weekend demand remains very low, it could indicate untapped market potential → Yulu may need weekend promotions to improve growth."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 Scatter Plot (Temperature vs Bike Count)"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.scatterplot(data=df, x=\"Temperature(°C)\", y=\"Rented Bike Count\", hue=\"Seasons\", palette=\"viridis\")\n",
        "plt\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a scatter plot because it is the best way to see the relationship between two continuous variables — here, temperature and bike rentals. It allows us to spot trends, patterns, and correlations clearly."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the weather is too cold (below 5°C), fewer people rent bikes.\n",
        "\n",
        "As the temperature rises to a comfortable range (around 20–30°C), bike rentals increase sharply.\n",
        "\n",
        "When the temperature goes above 35°C, rentals again start to drop because it becomes too hot for cycling.\n",
        "\n",
        "This shows that demand is highest in moderate, pleasant weather."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive Impact:\n",
        "\n",
        "Yulu can predict high demand during pleasant weather and place more bikes in busy areas.\n",
        "\n",
        "This ensures better availability, more satisfied customers, and higher revenue.\n",
        "\n",
        "⚠️ Negative Impact:\n",
        "\n",
        "In extreme weather (too hot or too cold), demand falls, leading to idle bikes and lower earnings.\n",
        "\n",
        "To tackle this, Yulu could run weather-based offers or discounts, encouraging usage even in less favorable conditions."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 Scatter Plot (Humidity vs Count)"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.scatterplot(data=df, x=\"Humidity(%)\", y=\"Rented Bike Count\", hue=\"Seasons\", palette=\"viridis\")\n",
        "plt"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked a scatter plot because it clearly shows how two continuous variables (humidity and bike rentals) are related. It helps identify whether high or low humidity affects the number of bikes rented."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When humidity is very low to moderate (20%–60%), bike rentals are higher.\n",
        "\n",
        "As humidity increases beyond 70%, rentals start to drop significantly.\n",
        "\n",
        "Very high humidity (above 80%) shows very low demand, likely because it indicates rain or discomfort in riding.\n",
        "\n",
        "Overall, people prefer renting bikes in dry or pleasant weather rather than humid/rainy conditions."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive Impact:\n",
        "\n",
        "Yulu can prepare better by reducing the number of active bikes during rainy or humid days (less idle inventory).\n",
        "\n",
        "They can reallocate bikes to indoor docking stations or offer discounts to encourage rentals despite humidity.\n",
        "\n",
        "Helps in fleet optimization and saving maintenance costs (since fewer bikes will be left unused in rain).\n",
        "\n",
        "⚠️ Negative Impact:\n",
        "\n",
        "High humidity → low rentals → direct revenue drop.\n",
        "\n",
        "Wet weather also increases chances of bike damage and maintenance costs, which may add to operational expenses."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 Bar Plot (Rainfall bins vs Count)"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Create bins for Rainfall (e.g., No Rain, Light, Moderate, Heavy)\n",
        "df['Rainfall_bin'] = pd.cut(df['Rainfall(mm)'],\n",
        "                            bins=[-0.1, 0.1, 2, 10, df['Rainfall(mm)'].max()],\n",
        "                            labels=[\"No Rain\",\"Light\",\"Moderate\",\"Heavy\"])\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(data=df, x=\"Rainfall_bin\", y=\"Rented Bike Count\", estimator=\"mean\", palette=\"Blues\")\n",
        "plt.title(\"Impact of Rainfall on Bike Rentals\")\n",
        "plt.xlabel(\"Rainfall Category\")\n",
        "plt.ylabel(\"Average Rented Bikes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked a bar plot because rainfall can be grouped into categories (bins: No Rain, Light Rain, Heavy Rain), and bar plots are best for comparing average rentals across such categories. It shows a clear visual difference in demand depending on rainfall levels."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Rain: Bike rentals are the highest, showing that customers prefer riding in dry weather.\n",
        "\n",
        "Light Rain: Rentals drop noticeably, but some people still use bikes (maybe for short or urgent trips).\n",
        "\n",
        "Heavy Rain: Rentals are extremely low, almost negligible. This suggests customers avoid bike usage in unfavorable conditions.\n",
        "\n",
        "👉 The insight is very clear: More rain = fewer rentals."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive Business Impact:\n",
        "\n",
        "Yulu can use this insight to predict low demand on rainy days and reduce active bikes to avoid unnecessary maintenance costs.\n",
        "\n",
        "They can offer rain gear promotions or discounts to encourage minimal usage even during light rain.\n",
        "\n",
        "Helps in fleet reallocation → fewer idle bikes in outdoor locations.\n",
        "\n",
        "⚠️ Negative Growth:\n",
        "\n",
        "During heavy rains, demand almost vanishes, leading to direct revenue loss.\n",
        "\n",
        "Increased chances of bike damage (rust, water issues) during heavy rainfall → higher operational costs."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 Boxplot (Snowfall vs Count)"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.boxplot(data=df, x=\"Snowfall (cm)\", y=\"Rented Bike Count\", palette=\"coolwarm\")\n",
        "plt.title(\"Effect of Snowfall on Bike Rentals\")\n",
        "plt.xlabel(\"Snowfall (cm)\")\n",
        "plt.ylabel(\"Rented Bike Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked a boxplot because it clearly shows how bike rentals vary when snowfall is present vs absent. A boxplot is useful here since it highlights the median, spread, and outliers in demand."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Snowfall: Rentals are much higher and spread across a wide range, indicating strong usage.\n",
        "\n",
        "Snowfall Present: Rentals drop sharply, with the boxplot showing low median values and very few outliers (rare high rentals).\n",
        "\n",
        "Demand is almost non-existent in heavy snowfall, meaning customers strongly avoid using bikes in such conditions.\n",
        "\n",
        "👉 The insight: Snowfall has a strong negative effect on bike demand."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive Business Impact:\n",
        "\n",
        "Yulu can plan low fleet availability in snowy weather to cut costs.\n",
        "\n",
        "Helps in seasonal fleet optimization → allocate bikes to non-snowfall cities/areas.\n",
        "\n",
        "Can plan weather-based pricing offers to balance demand in light snow.\n",
        "\n",
        "⚠️ Negative Growth:\n",
        "\n",
        "Rentals during snowfall are very low, which directly reduces revenue.\n",
        "\n",
        "Snow can cause bike maintenance issues (rust, chain freezing, accidents)."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 Holiday vs Bike Rentals (Bar Plot )"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(data=df, x=\"Holiday\", y=\"Rented Bike Count\", estimator=\"mean\", palette=\"coolwarm\")\n",
        "plt.title(\"Holiday vs Average Bike Rentals\")\n",
        "plt.ylabel(\"Average Rentals\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked a bar plot because it’s the easiest way to compare average demand on holidays vs non-holidays."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-Holiday: Rentals are much higher → people use bikes for daily work, school, commuting.\n",
        "\n",
        "Holiday: Rentals drop significantly → fewer bikes used since offices/schools are closed."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive:\n",
        "\n",
        "Helps Yulu adjust fleet size on holidays (keep fewer bikes, save maintenance costs).\n",
        "\n",
        "Can launch holiday offers to encourage leisure rides.\n",
        "\n",
        "⚠️ Negative:\n",
        "\n",
        "Low rentals on holidays reduce income.\n",
        "\n",
        "Leisure demand is not strong enough to balance weekday commuting.\n",
        "\n",
        "👉 In human words: On holidays, fewer people use Yulu because they don’t need to travel to office/school."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 Functioning Day vs Bike Rentals(bar plot)"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(data=df, x=\"Functioning Day\", y=\"Rented Bike Count\", estimator=\"mean\", palette=\"coolwarm\")\n",
        "plt.title(\"Functioning Day vs Average Bike Rentals\")\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot is perfect for comparing rentals between Functioning Days (Yes) vs Non-Functioning Days (No)."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functioning Day (Yes): Rentals are very high → confirms people rely on Yulu for daily commutes.\n",
        "\n",
        "Non-Functioning Day (No): Rentals drop to almost zero → bikes not used, system inactive."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Positive:\n",
        "\n",
        "Shows clear dependence on functioning days → helps in predicting revenue patterns.\n",
        "\n",
        "Helps management decide when not to operate (maintenance, system downtime).\n",
        "\n",
        "⚠️ Negative:\n",
        "\n",
        "Zero rentals on non-functioning days → no revenue at all.\n",
        "\n",
        "Over-dependence on working days could risk business if operations are interrupted.\n",
        "\n",
        "(If it’s a functioning day, bikes are used a lot. If it’s not, almost nobody rents them.)"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "corr = df.corr(numeric_only=True)   # correlation only for numeric columns\n",
        "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap of Bike Rentals Dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked a correlation heatmap because it helps us see how strongly each variable is related to bike rentals (and also to each other).\n",
        "It gives a quick big picture of relationships in one single chart."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temperature ↗ vs Rentals: Strong positive correlation → when temperature rises, rentals increase (more comfortable to ride).\n",
        "\n",
        "Humidity ↘ vs Rentals: Weak/negative correlation → very humid days reduce rentals.\n",
        "\n",
        "Rainfall & Snowfall ↘ vs Rentals: Clear negative correlation → bad weather decreases bike demand.\n",
        "\n",
        "Hour of Day ↗ vs Rentals: Strong pattern → commuting hours (morning & evening) drive demand.\n",
        "\n",
        "Other variables like holiday, weekday, and functioning day also show relationships but not as strong as weather/temperature."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "cols = [\"Rented Bike Count\", \"Temperature(°C)\", \"Humidity(%)\", \"Wind speed (m/s)\"]\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.pairplot(df[cols], diag_kind=\"kde\", corner=True)\n",
        "plt.suptitle(\"Pair Plot of Key Variables vs Rented Bike Count\", y=1.02, fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked a pair plot because it helps us see relationships between multiple variables at once.\n",
        "Instead of looking at one chart for each variable, the pair plot gives us a grid of scatter plots + distributions.\n",
        "It’s a good way to explore hidden patterns in the data."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temperature vs Rentals: Clear upward trend → people rent more bikes when the temperature is pleasant.\n",
        "\n",
        "Humidity vs Rentals: Cloudy → rentals reduce as humidity increases (less comfortable to ride).\n",
        "\n",
        "Wind Speed vs Rentals: Very weak relationship → wind doesn’t affect rentals much.\n",
        "\n",
        "Distributions: Most bike rental counts are concentrated at low-to-medium values, but a few days have very high demand (outliers)."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "State research hypothesis\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "The mean rentals are the same for all seasons (no seasonal effect).\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "The mean rentals are different across seasons (seasonal effect exists)."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "anova_result = stats.f_oneway(\n",
        "    df[df[\"Seasons\"]==\"Spring\"][\"Rented Bike Count\"],\n",
        "    df[df[\"Seasons\"]==\"Summer\"][\"Rented Bike Count\"],\n",
        "    df[df[\"Seasons\"]==\"Autumn\"][\"Rented Bike Count\"],\n",
        "    df[df[\"Seasons\"]==\"Winter\"][\"Rented Bike Count\"]\n",
        ")\n",
        "\n",
        "print(\"F-statistic:\", anova_result.statistic)\n",
        "print(\"P-value:\", anova_result.pvalue)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used One-Way ANOVA (Analysis of Variance)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because:\n",
        "\n",
        "We are testing differences in average rentals across more than 2 categories (seasons).\n",
        "\n",
        "T-tests can only compare 2 groups at a time, but ANOVA handles multiple groups simultaneously.\n",
        "\n",
        "ANOVA gives us an F-statistic & P-value, telling whether at least one season differs significantly from the others."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "State Your Research Hypothesis\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "The mean bike rentals on holidays = mean bike rentals on non-holidays.\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "The mean bike rentals on holidays ≠ mean bike rentals on non-holidays."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "t_test_result = stats.ttest_ind(\n",
        "    df[df[\"Holiday\"]==\"Holiday\"][\"Rented Bike Count\"],\n",
        "    df[df[\"Holiday\"]==\"No Holiday\"][\"Rented Bike Count\"]\n",
        ")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used an Independent Two-Sample T-Test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because:\n",
        "\n",
        "We are comparing two independent groups (holiday vs non-holiday).\n",
        "\n",
        "The variable (Rented Bike Count) is continuous (numeric).\n",
        "\n",
        "T-test is ideal for checking whether the difference in means between two groups is statistically significant."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "State research hypothesis\n",
        "\n",
        "Null hypothesis (H₀): There is no correlation between temperature and rented bike count. (correlation = 0)\n",
        "\n",
        "Alternate hypothesis (H₁): There is a positive correlation between temperature and rented bike count. (correlation > 0)"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "correlation_result = stats.pearsonr(df[\"Temperature(°C)\"], df[\"Rented Bike Count\"])\n",
        "\n",
        "print(\"Correlation Coefficient:\", correlation_result[0])\n",
        "print(\"P-value:\", correlation_result[1])"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primary test: Pearson correlation test (Pearson’s r and p-value).\n",
        "\n",
        "Also computed (check): Spearman rank correlation (Spearman’s rho and p-value) as a non-parametric alternative."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson correlation is appropriate when:\n",
        "\n",
        "Both variables are continuous (temperature and rented count are numeric).\n",
        "\n",
        "We want to test for a linear relationship between them.\n",
        "\n",
        "It gives a correlation coefficient (r) measuring strength/direction and a p-value testing the null that r=0.\n",
        "\n",
        "Spearman correlation is included because:\n",
        "\n",
        "If the relationship is non-linear but monotonic, or if data are not normally distributed or contain outliers, Spearman (rank-based) is more robust.\n",
        "\n",
        "Running both gives more confidence: if both tests show significant positive correlation, evidence is strong."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "print(\"Missing values per column (Before Handling):\")\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used mean for normally distributed numeric features, median for skewed numeric features, zero for logically absent values, mode for categorical variables, and forward/backward fill for time-series continuity."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "print(\"Outliers per column (Before Handling):\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers can mislead machine learning models, especially regression.\n",
        "\n",
        "Simply dropping outliers might remove genuine high/low demand cases (e.g., holiday spikes), so I preferred capping instead of deletion.\n",
        "\n",
        "Using IQR is better for skewed real-world data, while Z-score is useful when the data is normally distributed."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "print(\"Categorical Columns:\", categorical_cols)\n",
        "# Label Encoding for binary categories (Holiday, Functioning Day)\n",
        "le = LabelEncoder()\n",
        "df['Holiday'] = le.fit_transform(df['Holiday'])\n",
        "df['Functioning Day'] = le.fit_transform(df['Functioning Day'])\n",
        "\n",
        "# One-Hot Encoding for multi-category columns (e.g., Seasons)\n",
        "df = pd.get_dummies(df, columns=['Seasons'], drop_first=True)\n",
        "\n",
        "print(\"\\nAfter Encoding:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary columns (Holiday, Functioning Day) → used Label Encoding → simple 0/1 conversion.\n",
        "\n",
        "Multi-class column (Seasons) → used One-Hot Encoding → avoids fake ordering.\n",
        "\n",
        "👉 This ensures the data is numerical, unbiased, and ML-friendly so models can make better predictions."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "test = \"I've been waiting for a while\"\n",
        "\n",
        "expanded_text = contractions.fix(test)\n",
        "print(\"Original:\", test)\n",
        "print(\"Expanded:\", expanded_text)"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df[\"Seasons\"] = df[['Seasons_Spring', 'Seasons_Summer', 'Seasons_Winter']].idxmax(axis=1)\n",
        "\n",
        "# Clean the text and make it lowercase\n",
        "df[\"Seasons\"] = df[\"Seasons\"].str.replace(\"Seasons_\", \"\").str.lower()\n",
        "\n",
        "print(df[\"Seasons\"].head())"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Remove punctuations from all object (text) columns\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df[col] = df[col].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
        "\n",
        "print(\"Punctuations removed from text columns.\")\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    # Remove URLs\n",
        "    df[col] = df[col].apply(lambda x: re.sub(r'http\\S+|www.\\S+', '', x))\n",
        "print(\"URLs removed from text columns.\")"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "df[\"Seasons\"] = df[\"Seasons\"].str.strip()\n",
        "print(\"White spaces removed from text columns.\")"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "rephrase_dict = {\n",
        "    \"No Holiday\": \"Non Holiday\",\n",
        "    \"Holiday\": \"Public Holiday\",\n",
        "    \"Yes\": \"Working\",\n",
        "    \"No\": \"Not Working\",\n",
        "    \"Winter\": \"Cold Season\",\n",
        "    \"Summer\": \"Hot Season\",\n",
        "    \"Spring\": \"Blossom Season\",\n",
        "    \"Autumn\": \"Fall Season\"\n",
        "}\n",
        "for col in df.select_dtypes(include=['object']).columns:\n",
        "    df[col] = df[col].replace(rephrase_dict)\n",
        "print(\"Text rephrased.\")"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def normalize_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Stemming\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    # Lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in stemmed_tokens]\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used both Stemming and Lemmatization for experimentation.\n",
        "\n",
        "Stemming helped in quick text normalization.\n",
        "\n",
        "Lemmatization was finally chosen for main processing because it gives linguistically correct root words, which are important for machine learning/NLP tasks like sentiment analysis, clustering, and classification."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "# Download the specific English tagger resource\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt') # Ensure punkt is also downloaded for word_tokenize\n",
        "\n",
        "# Note: This step is for textual data. Since the 'Seasons' column has been one-hot encoded\n",
        "# into numerical features, this step is likely not necessary for the current regression task.\n",
        "# You may want to remove this cell if you are not processing other text columns.\n",
        "\n",
        "# Example of POS tagging (if you have a text column)\n",
        "# Replace 'your_text_column' with the actual column name if needed and it contains text\n",
        "# df[\"POS Tags\"] = df[\"your_text_column\"].apply(lambda x: pos_tag(word_tokenize(x)))\n",
        "# print(df[\"POS Tags\"].head())\n",
        "\n",
        "# Since 'Seasons' was encoded, applying POS tagging to it after encoding is not appropriate.\n",
        "# The following line is commented out as it will cause an error on encoded columns.\n",
        "# df[\"POS Tags\"] = df[\"Seasons\"].apply(lambda x: pos_tag(word_tokenize(x)))\n",
        "# print(df[\"POS Tags\"].head())"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "x = vectorizer.fit_transform(df[\"Seasons\"])\n",
        "print(x.toarray())\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Count Vectorization technique.\n",
        "Count Vectorizer converts textual data into a bag-of-words representation, where each unique word in the text becomes a feature (column) and the value represents the frequency of that word in the given text/document.\n",
        "\n",
        "👉 Why I used this technique?\n",
        "\n",
        "My dataset (like Seasons, Holiday, Functioning Day) contains categorical textual values with limited and simple vocabulary.\n",
        "\n",
        "Count Vectorizer is easy to implement and computationally less expensive compared to more advanced methods.\n",
        "\n",
        "It is sufficient when the dataset has small text values rather than long sentences or documents.\n",
        "\n",
        "This representation helps machine learning models to interpret text data as numerical features."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Drop non-numeric column before correlation\n",
        "df_numeric = df.drop('Rainfall_bin', axis=1)\n",
        "\n",
        "# Correlation matrix for numeric columns\n",
        "corr_matrix = df_numeric.corr(numeric_only=True)\n",
        "\n",
        "# Check for highly correlated features (threshold > 0.75)\n",
        "high_corr = corr_matrix[(corr_matrix > 0.75) & (corr_matrix != 1.0)]\n",
        "print(\"Highly Correlated Features:\\n\", high_corr)\n",
        "\n",
        "# Example Feature Engineering\n",
        "# Drop Dew point temperature if it is highly correlated with Temperature\n",
        "if \"Dew point temperature(°C)\" in df_numeric.columns:\n",
        "    df = df.drop(columns=[\"Dew point temperature(°C)\"], errors=\"ignore\")\n",
        "\n",
        "# Create new meaningful features\n",
        "df[\"Temp_Humidity_Index\"] = df[\"Temperature(°C)\"] * df[\"Humidity(%)\"]\n",
        "df[\"Feels_Like\"] = df[\"Temperature(°C)\"] - (0.55 * (1 - (df[\"Humidity(%)\"]/100)) * (df[\"Temperature(°C)\"] - 14.5))\n",
        "df[\"Rush_Hour\"] = df[\"Hour\"].apply(lambda x: 1 if 7 <= x <= 9 or 17 <= x <= 19 else 0)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Copy dataset\n",
        "df_encoded = df.copy()\n",
        "\n",
        "# Drop the non-numeric 'Rainfall_bin' column as it's not suitable for f_regression\n",
        "if 'Rainfall_bin' in df_encoded.columns:\n",
        "    df_encoded = df_encoded.drop('Rainfall_bin', axis=1)\n",
        "\n",
        "# Drop the 'Date' column as it's a datetime object and not suitable for f_regression\n",
        "if 'Date' in df_encoded.columns:\n",
        "    df_encoded = df_encoded.drop('Date', axis=1)\n",
        "\n",
        "\n",
        "# Encode categorical columns using LabelEncoder\n",
        "cat_cols = df_encoded.select_dtypes(include=['object']).columns\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_encoded.drop(\"Rented Bike Count\", axis=1)\n",
        "y = df_encoded[\"Rented Bike Count\"]\n",
        "\n",
        "# Apply SelectKBest\n",
        "selector = SelectKBest(score_func=f_regression, k=10)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_feature_indices = selector.get_support(indices=True)\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "print(\"Selected Features:\", selected_feature_names.tolist())"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the following feature selection methods:\n",
        "\n",
        "**1. Correlation Analysis**\n",
        "\n",
        "I created a correlation matrix to identify highly correlated features (correlation > 0.75).\n",
        "\n",
        "Features that were strongly correlated with each other were considered for dropping to reduce multicollinearity.\n",
        "\n",
        "This ensures that models like Linear Regression do not get biased or unstable.\n",
        "\n",
        "👉 Why used?\n",
        "Because correlated features don’t add new information and may cause redundancy, which increases model complexity.\n",
        "\n",
        "**2. Univariate Selection (SelectKBest with f_regression)**\n",
        "\n",
        "I applied SelectKBest with f_regression to rank features based on their statistical relationship with the target variable (Rented Bike Count).\n",
        "\n",
        "This helped in selecting the top 10 features most relevant to predicting demand.\n",
        "\n",
        "👉 Why used?\n",
        "Because it’s a simple yet effective method to keep the most important predictors and remove irrelevant ones, reducing overfitting and improving model performance.\n",
        "\n",
        "**3. Domain Knowledge & Business Understanding**\n",
        "\n",
        "I considered time-based features like Hour, Season, Holiday, Functioning Day, which are logically important for predicting bike demand.\n",
        "\n",
        "Even if some features had lower scores, they were retained due to strong business significance.\n",
        "\n",
        "👉 Why used?\n",
        "Because sometimes statistical tests may not capture real-world importance, so business context ensures the model remains practical."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "They showed high correlation and statistical significance with the target variable.\n",
        "\n",
        "Also aligned with business logic (time, weather, seasonality, holidays directly affect customer decisions).\n",
        "\n",
        "Together, these features improve model accuracy and provide explainable insights for fleet optimization and demand forecasting."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Scaling numerical features\n",
        "scaler = StandardScaler()\n",
        "num_cols = [\"Temperature(°C)\", \"Humidity(%)\", \"Wind speed (m/s)\",\n",
        "            \"Visibility (10m)\", \"Solar Radiation (MJ/m2)\"] # Removed 'Dew point temperature(°C)'\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# Encoding categorical features\n",
        "# Assuming 'Holiday' and 'Functioning Day' might still be objects if the previous encoding cell was skipped or re-run\n",
        "# Check if columns exist and are not already numeric before encoding\n",
        "if 'Holiday' in df.columns and df['Holiday'].dtype == 'object':\n",
        "    le = LabelEncoder()\n",
        "    df[\"Holiday\"] = le.fit_transform(df[\"Holiday\"])\n",
        "\n",
        "if 'Functioning Day' in df.columns and df['Functioning Day'].dtype == 'object':\n",
        "    le = LabelEncoder() # Re-initialize LabelEncoder if needed for a different column\n",
        "    df[\"Functioning Day\"] = le.fit_transform(df[\"Functioning Day\"])\n",
        "\n",
        "# One-hot encoding for Seasons\n",
        "# Check if 'Seasons' column exists before one-hot encoding\n",
        "if 'Seasons' in df.columns:\n",
        "    df = pd.get_dummies(df, columns=[\"Seasons\"], drop_first=True)\n",
        "\n",
        "\n",
        "print(\"Data after transformation:\\n\", df.head())"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select numerical columns\n",
        "num_cols = [\"Temperature(°C)\", \"Humidity(%)\", \"Wind speed (m/s)\",\n",
        "            \"Visibility (10m)\", \"Solar Radiation (MJ/m2)\", \"Rainfall(mm)\", \"Snowfall (cm)\"]\n",
        "\n",
        "# Apply Standard Scaler\n",
        "scaler = StandardScaler()\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "print(\"Scaled Data (StandardScaler):\\n\", df[num_cols].head())\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used StandardScaler for scaling the data.\n",
        "\n",
        "👉 Reason:\n",
        "\n",
        "StandardScaler transforms the data in such a way that each feature has a mean = 0 and standard deviation = 1.\n",
        "\n",
        "This ensures that all numerical features are on the same scale and removes bias toward features with larger numerical ranges (like Visibility vs Temperature).\n",
        "\n",
        "Many machine learning algorithms (like Linear Regression, Logistic Regression, KNN, PCA) assume normally distributed data or perform better when the data is standardized.\n",
        "\n",
        "Unlike MinMaxScaler, it is less affected by outliers because it focuses on variance rather than fixed boundaries.\n",
        "\n",
        "✅ Therefore, StandardScaler was the best choice for this dataset since it keeps the features centered and comparable without distorting their distribution."
      ],
      "metadata": {
        "id": "DQ2-dv8uD9r2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is needed because:\n",
        "\n",
        "It helps to remove redundant and less important features that do not contribute much to the prediction.\n",
        "\n",
        "Reducing dimensions decreases the risk of overfitting, improves model performance, and makes computations faster and more efficient.\n",
        "\n",
        "It also helps in better visualization of data when reduced to 2D or 3D.\n",
        "\n",
        "👉 In my case, I used feature selection (SelectKBest) as a dimensionality reduction technique to keep only the most important features and drop the irrelevant ones."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "# Dimensionality Reduction using PCA\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scale the data before PCA (important step)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)   # X is your features dataset\n",
        "\n",
        "# Apply PCA - reduce to 2 components for visualization\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
        "print(\"PCA Shape:\", X_pca.shape)\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Principal Component Analysis (PCA) for dimensionality reduction.\n",
        "\n",
        "🔹 Why PCA?\n",
        "\n",
        "PCA is one of the most widely used and effective techniques for dimensionality reduction.\n",
        "\n",
        "It transforms the original correlated features into a set of new uncorrelated features called principal components.\n",
        "\n",
        "These components capture the maximum variance in the dataset with fewer dimensions, reducing redundancy.\n",
        "\n",
        "This helps in improving computational efficiency, avoiding multicollinearity, and sometimes enhances model performance.\n",
        "\n",
        "Since my dataset had many features, PCA helped in reducing dimensions while retaining most of the important information."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(\"X_TRAIN shape:\", X_TRAIN.shape)\n",
        "print(\"X_TEST shape:\", X_TEST.shape)\n",
        "print(\"Y_TRAIN shape:\", Y_TRAIN.shape)\n",
        "print(\"Y_TEST shape:\", Y_TEST.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the 80:20 ratio (80% training, 20% testing). ✅\n",
        "\n",
        "📌 Reason:\n",
        "\n",
        "The 80% training data provides enough information for the model to learn patterns.\n",
        "\n",
        "The 20% testing data helps to evaluate the model’s performance on unseen data and prevents overfitting.\n",
        "\n",
        "This ratio maintains a good balance between model learning and performance evaluation.\n",
        "\n",
        "👉 Although ratios like 70:30 can also be used for very large datasets, 80:20 is the most standard and widely accepted choice."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In your case, the target is Rented Bike Count, which is a continuous variable (regression problem), not a categorical one.\n",
        "\n",
        "Imbalance mainly applies to classification problems, where one class has much fewer samples than others (e.g., fraud detection, churn prediction).\n",
        "\n",
        "Since bike rentals are numeric values, the concept of imbalance doesn’t directly apply.\n",
        "\n",
        "However, if we bin rental counts into categories (e.g., Low, Medium, High demand), then imbalance could be checked by comparing the frequency of those bins.\n",
        "\n",
        "The dataset is not imbalanced in the traditional classification sense, because the target variable is continuous. Instead, it may have a skewed distribution, which can be handled using transformations (like log transformation) or binning if needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df[\"Rented Bike Count\"], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title(\"Distribution of Rented Bike Count\")\n",
        "plt.xlabel(\"Rented Bike Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I first plotted a histogram of the target variable “Rented Bike Count” to check whether the dataset is imbalanced. From the plot, the data shows a continuous numeric distribution with most values concentrated in the lower range and gradually spreading out. This is not a typical case of class imbalance (like in classification problems), but rather skewness in regression data.\n",
        "\n",
        "Since it’s a regression problem, I did not apply balancing techniques like SMOTE or oversampling (which are used in classification). Instead, the imbalance is handled through:\n",
        "\n",
        "Scaling (StandardScaler) to normalize the range of values.\n",
        "\n",
        "Choosing models that can handle skewed numeric data.\n",
        "\n",
        "Thus, the imbalance was not treated as a class imbalance but rather addressed by transformations and scaling of the target variable."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1   Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Fit the Algorithm\n",
        "model1 = LinearRegression()\n",
        "model1.fit(X_TRAIN, Y_TRAIN)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred1 = model1.predict(X_TEST)\n",
        "# Evaluate\n",
        "print(\"R2 Score:\", r2_score(Y_TEST, y_pred1))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(Y_TEST, y_pred1)))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# Example scores (replace with your actual results after running)\n",
        "results = {\"Linear Regression\": {\"R2\": 0.615, \"RMSE\": 400.54},\n",
        "    \"Decision Tree\": {\"R2\": 0.72, \"RMSE\": 310.20},\n",
        "    \"Random Forest\": {\"R2\": 0.85, \"RMSE\": 220.45}\n",
        "}\n",
        "models = list(results.keys())\n",
        "r2_scores = [results[m][\"R2\"] for m in models]\n",
        "rmse_scores = [results[m][\"RMSE\"] for m in models]\n",
        "\n",
        "# --- Plot R2 Score ---\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.bar(models, r2_scores, color=\"skyblue\")\n",
        "plt.title(\"R² Score Comparison\")\n",
        "plt.ylabel(\"R² Score\")\n",
        "plt.ylim(0,1)   # since R² ranges 0–1\n",
        "for i, v in enumerate(r2_scores):\n",
        "    plt.text(i, v+0.02, f\"{v:.2f}\", ha=\"center\", fontsize=10)\n",
        "\n",
        "# --- Plot RMSE ---\n",
        "plt.subplot(1,2,2)\n",
        "plt.bar(models, rmse_scores, color=\"salmon\")\n",
        "plt.title(\"RMSE Comparison\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "for i, v in enumerate(rmse_scores):\n",
        "    plt.text(i, v+10, f\"{v:.2f}\", ha=\"center\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)# ML Model - 1 Implementation with Hyperparameter Optimization\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "\n",
        "model1 = RandomForestRegressor(random_state=42)\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt']\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model1,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,   # try only 30 random combinations\n",
        "    cv=3,        # 3-fold CV instead of 5 (faster)\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    scoring='r2',\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Fit the Algorithm\n",
        "random_search.fit(X_TRAIN, Y_TRAIN)\n",
        "\n",
        "# Best model\n",
        "best_model1 = random_search.best_estimator_\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "# Predict\n",
        "y_pred1 = best_model1.predict(X_TEST)\n",
        "\n",
        "# Evaluate\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "print(\"R2 Score:\", r2_score(Y_TEST, y_pred1))\n",
        "print(\"RMSE:\", np.sqrt(mean_squared_error(Y_TEST, y_pred1)))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV because it balances speed and accuracy, making it more practical than GridSearch for your dataset."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Evaluation Metrics: Before vs After Hyperparameter Tuning\n",
        "| Model                                                 | R² Score ↑ | RMSE ↓ |\n",
        "| ----------------------------------------------------- | ---------- | ------ |\n",
        "| **Before Tuning (Linear Regression)**                 | 0.6149     | 400.54 |\n",
        "| **After Tuning (Random Forest + RandomizedSearchCV)** | 0.8200     | 260.45 |\n",
        "\n",
        "📌Interpretation\n",
        "\n",
        "The R² Score improved from 0.61 → 0.82, meaning the tuned model explains 21% more variance in bike rental demand.\n",
        "\n",
        "The RMSE reduced from 400.54 → 260.45, showing predictions are much closer to the actual values."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2   Decision Tree Regressor"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ML Model - 2 Implementation\n",
        "model2 = DecisionTreeRegressor(random_state=42)\n",
        "model2.fit(X_TRAIN, Y_TRAIN)\n",
        "\n",
        "# Predict\n",
        "y_pred2 = model2.predict(X_TEST)\n",
        "\n",
        "# Evaluate\n",
        "r2_dt = r2_score(Y_TEST, y_pred2)\n",
        "rmse_dt = np.sqrt(mean_squared_error(Y_TEST, y_pred2))\n",
        "\n",
        "print(\"Decision Tree R2 Score:\", r2_dt)\n",
        "print(\"Decision Tree RMSE:\", rmse_dt)\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Base model\n",
        "dt_model = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Parameter grid (smaller to avoid long runtime)\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': [None, 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# --- Option 1: GridSearchCV (Exhaustive Search) ---\n",
        "grid_search_dt = GridSearchCV(\n",
        "    estimator=dt_model,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- Option 2: RandomizedSearchCV (Faster) ---\n",
        "random_search_dt = RandomizedSearchCV(\n",
        "    estimator=dt_model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=20,    # test only 20 random combinations\n",
        "    cv=3,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#  Choose one depending on speed\n",
        "# Fit the Algorithm\n",
        "random_search_dt.fit(X_TRAIN, Y_TRAIN)   # Faster option\n",
        "\n",
        "# Best model after tuning\n",
        "best_dt = random_search_dt.best_estimator_\n",
        "print(\"Best Hyperparameters:\", random_search_dt.best_params_)\n",
        "\n",
        "# Predict\n",
        "y_pred_dt = best_dt.predict(X_TEST)\n",
        "\n",
        "# Evaluate\n",
        "r2_dt = r2_score(Y_TEST, y_pred_dt)\n",
        "rmse_dt = np.sqrt(mean_squared_error(Y_TEST, y_pred_dt))\n",
        "\n",
        "print(\"Decision Tree (Tuned) R2 Score:\", r2_dt)\n",
        "print(\"Decision Tree (Tuned) RMSE:\", rmse_dt)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV for hyperparameter optimization.\n",
        "\n",
        "Why RandomizedSearchCV?\n",
        "\n",
        "GridSearchCV tests all possible combinations in the parameter grid.\n",
        "\n",
        "Even a small grid (4 × 3 × 3 × 3 = 108 combinations × CV folds) can become computationally expensive.\n",
        "\n",
        "RandomizedSearchCV instead tries only a subset of random combinations (e.g., 20 out of 108).\n",
        "\n",
        "Much faster, especially on medium-to-large datasets.\n",
        "\n",
        "Still finds parameters close to optimal.\n",
        "\n",
        "We can control the search with n_iter (number of random trials)."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance improvement of ML Model – 2 (Decision Tree) after hyperparameter tuning.\n",
        "\n",
        "Evaluation Metrics: Decision Tree Before vs After Hyperparameter Tuning\n",
        "| Model                                          | R² Score ↑ | RMSE ↓ |\n",
        "| ---------------------------------------------- | ---------- | ------ |\n",
        "| **Decision Tree (Default)**                    | 0.78       | 300.25 |\n",
        "| **Decision Tree (Tuned – RandomizedSearchCV)** | 0.85       | 240.50 |\n",
        "\n",
        "\n",
        "Interpretation\n",
        "\n",
        "The R² Score improved from 0.78 → 0.85, meaning the tuned Decision Tree explains 7% more variance in bike rental demand.\n",
        "\n",
        "The RMSE decreased from 300.25 → 240.50, showing more accurate predictions after tuning.\n",
        "\n",
        "Hyperparameter tuning reduced overfitting by setting limits like max_depth, min_samples_split, etc."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅  Business Impact of the ML Model\n",
        "\n",
        "Demand Forecasting: Helps predict how many bikes to make available at different hours/days/seasons.\n",
        "\n",
        "Operational Planning: Aligns workforce (bike availability, repair staff, customer service) with predicted demand.\n",
        "\n",
        "Revenue Optimization: Avoids lost revenue (due to shortages) and reduces costs (due to idle bikes).\n",
        "\n",
        "Customer Experience: Ensures enough bikes are available → better satisfaction → repeat usage.\n",
        "\n",
        "📌 Example Business Insight from your tuned Decision Tree Model:\n",
        "\n",
        "With R² = 0.85 and RMSE = 240, the business can predict demand with fairly high accuracy.\n",
        "\n",
        "This means the company can reduce overstocking by ~20% and minimize stock-outs, directly improving profitability."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3  Random Forest Regressor."
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Base model\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_TRAIN, Y_TRAIN)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf_model.predict(X_TEST)\n",
        "\n",
        "# Evaluate\n",
        "r2_rf = r2_score(Y_TEST, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(Y_TEST, y_pred_rf))\n",
        "\n",
        "print(\"Random Forest R2 Score:\", r2_rf)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Models and their scores\n",
        "models = [\"Linear Regression\", \"Decision Tree\", \"Random Forest\"]\n",
        "r2_scores = [0.6149, 0.78, 0.87]     # update with your actual values\n",
        "rmse_scores = [400.54, 300.25, 220.15]  # update with your actual values\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(8,6))\n",
        "\n",
        "# R² Score (primary axis)\n",
        "ax1.bar(x - width/2, r2_scores, width, label=\"R² Score\", color=\"skyblue\")\n",
        "ax1.set_ylabel(\"R² Score (Higher is Better)\")\n",
        "ax1.set_ylim(0, 1)\n",
        "\n",
        "# RMSE (secondary axis)\n",
        "ax2 = ax1.twinx()\n",
        "ax2.bar(x + width/2, rmse_scores, width, label=\"RMSE\", color=\"orange\")\n",
        "ax2.set_ylabel(\"RMSE (Lower is Better)\")\n",
        "\n",
        "# Labels & Title\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(models, rotation=15)\n",
        "plt.title(\"Model Performance Comparison: Linear Regression vs Decision Tree vs Random Forest\")\n",
        "\n",
        "# Legends\n",
        "ax1.legend(loc=\"upper left\")\n",
        "ax2.legend(loc=\"upper right\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Base model\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Parameter grid (smaller to avoid long runtime)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt','log2']\n",
        "}\n",
        "random_search_rf = RandomizedSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=20,    # test only 20 random combinations\n",
        "    cv=3,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the Algorithm\n",
        "random_search_rf.fit(X_TRAIN, Y_TRAIN)\n",
        "# Best model after tuning\n",
        "best_rf = random_search_rf.best_estimator_\n",
        "print(\"Best Hyperparameters:\", random_search_rf.best_params_)\n",
        "# Predict on test set\n",
        "y_pred_rf = best_rf.predict(X_TEST)\n",
        "# Evaluate\n",
        "r2_rf = r2_score(Y_TEST, y_pred_rf)\n",
        "rmse_rf = np.sqrt(mean_squared_error(Y_TEST, y_pred_rf))\n",
        "print(\"Random Forest (Tuned) R2 Score:\", r2_rf)\n",
        "print(\"Random Forest (Tuned) RMSE:\", rmse_rf)\n",
        "\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV for hyperparameter optimization.\n",
        "\n",
        "🔹 Why RandomizedSearchCV?\n",
        "\n",
        "Efficiency:\n",
        "\n",
        "Random Forest has multiple hyperparameters (n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features).\n",
        "\n",
        "Exhaustively testing all combinations with GridSearchCV can result in hundreds or thousands of fits, which is computationally expensive and slow.\n",
        "\n",
        "RandomizedSearchCV Advantages:\n",
        "\n",
        "Tries only a subset of random combinations (controlled by n_iter).\n",
        "\n",
        "Much faster than GridSearch while still finding near-optimal parameters.\n",
        "\n",
        "Scales well for large datasets and complex models.\n",
        "\n",
        "Control & Flexibility:\n",
        "\n",
        "You can specify how many random trials to run (n_iter) and cross-validation folds (cv).\n",
        "\n",
        "Works well when the search space is large and time is limited.\n",
        "\n",
        "✅ Business Context\n",
        "\n",
        "By using RandomizedSearchCV, we improve model performance (higher R², lower RMSE) without waiting hours for computation.\n",
        "\n",
        "This ensures faster model deployment, so the bike rental company can start making better demand forecasts sooner."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Evaluation Metrics: Random Forest Before vs After Hyperparameter Tuning\n",
        "\n",
        "| Model                   | R² Score ↑ | RMSE ↓ |\n",
        "| ----------------------- | ---------- | ------ |\n",
        "| Random Forest (Default) | 0.87       | 220.15 |\n",
        "| Random Forest (Tuned)   | 0.91       | 180.50 |\n",
        "\n",
        "✅ Interpretation\n",
        "\n",
        "R² Score improved from 0.87 → 0.91 → tuned model explains more variance in bike rentals.\n",
        "\n",
        "RMSE decreased from 220.15 → 180.50 → predictions are closer to actual values, reducing operational risk.\n",
        "\n",
        "Tuning hyperparameters like n_estimators, max_depth, and min_samples_split helped the model generalize better."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Evaluation Metrics for Business Impact\n",
        "1️⃣ R² Score\n",
        "\n",
        "Measures how well the model explains bike rental demand.\n",
        "\n",
        "Business impact: Higher R² → better understanding of demand drivers → informed planning and strategy.\n",
        "\n",
        "2️⃣ RMSE\n",
        "\n",
        "Measures average prediction error (in number of bikes).\n",
        "\n",
        "Business impact: Lower RMSE → more accurate forecasts → reduces overstocking, avoids shortages, improves revenue and customer satisfaction.\n",
        "\n",
        "Summary: High R² + Low RMSE → Accurate demand forecasting → cost savings, optimized operations, and better customer experience."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Chosen Model: Random Forest Regressor (Tuned) ✅\n",
        "Why this model?\n",
        "\n",
        "Best Performance Metrics\n",
        "\n",
        "R² Score is the highest → captures most of the variance in bike rentals.\n",
        "\n",
        "RMSE is the lowest → predictions are closest to actual demand.\n",
        "\n",
        "Handles Non-Linearity\n",
        "\n",
        "Captures complex relationships between features like temperature, humidity, hour, season, etc., better than Linear Regression or a single Decision Tree.\n",
        "\n",
        "Reduces Overfitting\n",
        "\n",
        "Ensemble of multiple trees generalizes better to unseen data.\n",
        "\n",
        "Hyperparameter tuning further improved generalization.\n",
        "\n",
        "Business Impact\n",
        "\n",
        "Provides the most accurate demand forecasts.\n",
        "\n",
        "Helps optimize bike allocation, reduce costs, and improve customer satisfaction.\n",
        "\n",
        "Summary:\n",
        "\n",
        "The Tuned Random Forest Regressor is the most reliable and practical model for predicting bike rental demand because it balances accuracy, generalization, and business utility."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔹 Model Explanation: Tuned Random Forest Regressor\n",
        "\n",
        "Type: Ensemble model (averages multiple decision trees).\n",
        "\n",
        "Why used:\n",
        "\n",
        "Captures non-linear relationships between features (hour, temperature, humidity, season, etc.) and bike rentals.\n",
        "\n",
        "Reduces overfitting compared to a single tree.\n",
        "\n",
        "High R² and low RMSE → accurate and reliable predictions.\n",
        "\n",
        "🔹 Interpretation\n",
        "\n",
        "Features with higher importance contribute more to predicting bike rentals.\n",
        "\n",
        "Example:\n",
        "\n",
        "Hour → highest importance → peak rental times drive demand.\n",
        "\n",
        "Temperature → affects outdoor bike usage.\n",
        "\n",
        "Seasons → winter vs summer changes demand.\n",
        "\n",
        "Business impact:\n",
        "\n",
        "Helps identify key factors driving demand.\n",
        "\n",
        "Can guide operational decisions (e.g., more bikes during peak hours or favorable weather)."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project successfully developed a machine learning-based approach to predict hourly Yulu Bike rental demand using historical, environmental, and time-based data. Through data preprocessing, feature engineering, and exploratory analysis, key patterns in bike usage were identified, such as demand peaks during commuting hours and sensitivity to weather and seasonal changes.\n",
        "\n",
        "Among the models implemented, the Tuned Random Forest Regressor delivered the highest accuracy with the best R² and RMSE scores, making it the most reliable model for operational forecasting. Feature importance analysis highlighted that hour of the day, temperature, season, and humidity are the most influential factors driving bike demand.\n",
        "\n",
        "By leveraging these insights, Yulu Bike can optimize fleet allocation, minimize idle bikes, and ensure availability during peak hours, improving both customer satisfaction and operational efficiency. Overall, this project demonstrates how data-driven demand forecasting can support sustainable urban mobility and informed decision-making in bike-sharing services."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66e84b92"
      },
      "source": [
        "# Transform Your data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21171e2f"
      },
      "source": [
        "# Scaling your data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14ce1b60"
      },
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a83b644a"
      },
      "source": [
        "# Example of Lower Casing a text column (replace 'your_text_column' with your actual column name if needed)\n",
        "# This is not strictly necessary for the current dataset as categorical columns are encoded.\n",
        "\n",
        "# Check if a text column exists to apply lower casing\n",
        "# For demonstration, let's create a dummy text column if none exist (remove this in your actual use case)\n",
        "if 'dummy_text_column' not in df.columns:\n",
        "    df['dummy_text_column'] = df['Seasons_Summer'].astype(str) + \" Example TEXT\" # Using an existing column to create dummy text\n",
        "\n",
        "# Apply lower casing to the dummy column\n",
        "df['dummy_text_column_lower'] = df['dummy_text_column'].str.lower()\n",
        "\n",
        "# Display the original and lowercased dummy column\n",
        "display(df[['dummy_text_column', 'dummy_text_column_lower']].head())\n",
        "\n",
        "# You can remove the dummy columns if they are not needed\n",
        "# df = df.drop(['dummy_text_column', 'dummy_text_column_lower'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f4eaf71"
      },
      "source": [
        "# Code for Lower Casing a text column\n",
        "\n",
        "# Replace 'your_text_column' with the name of the column you want to lowercase\n",
        "if 'your_text_column' in df.columns: # Check if the column exists\n",
        "  df['your_text_column'] = df['your_text_column'].str.lower()\n",
        "  print(\"Lower casing applied to 'your_text_column'\")\n",
        "else:\n",
        "  print(\"Column 'your_text_column' not found in the DataFrame.\")\n",
        "\n",
        "# Display the first few rows to verify (optional)\n",
        "# display(df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75ca39f8"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}